{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft and Experiment Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction:             163(xxx)\n",
      "Content:                  357(300)\n",
      "Innovation:               185(300)\n",
      "Technical Quality:        233(200)\n",
      "Application and X-factor: 212(200)\n",
      "Presentation:             114(100)\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from IPython.nbformat import current\n",
    "\n",
    "with io.open('A1_Report.ipynb', 'r', encoding='utf-8') as f:\n",
    "    nb = current.read(f, 'json')\n",
    "\n",
    "guide = {\n",
    "    'Introduction': 'xxx',\n",
    "    'Content': '300',\n",
    "    'Innovation': '300',\n",
    "    'Technical Quality': '200',\n",
    "    'Application and X-factor': '200',\n",
    "    'Presentation': '100'\n",
    "}\n",
    "\n",
    "for i in range(len(nb.worksheets[0].cells)):\n",
    "    cell = nb.worksheets[0].cells[i]\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        caption = nb.worksheets[0].cells[i-1].source\n",
    "        if caption in guide.keys():\n",
    "            print('{:<25} {:>3}({})'.format(\n",
    "                caption + ':',\n",
    "                len(cell['source'].replace('#', '').replace('$', '').lstrip().split(' ')),\n",
    "                guide[caption]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Report on \"[HOC97] Long Short-Term Memory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to github repository: https://github.com/pitwegner/UTS_ML2019_ID13691340"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytical methods to gain insight to sequential data are becoming increasingly important in today's world. Predictive time series data analytics and natural language processing are examples of this major development that we can see in many fields in the course of digitalization. As a student in a digital health degree, it is particularly interesting to look at applications in the medical domain. For example, evaluating medical observations such as vital signs and ECG, document analysis and categorization for electronic health records, or translations in speech and writing are huge-impact utilizations (NHS 2019). In my master's thesis I am going to tackle the challenge of classifiying nurse activities from motion sensors, also a kind of time series data. It is often analyzed by recurrent neural networks (RNNs), as they can hold a memory of past events via feedback loops. In this literature review, I will examine the recurrent network architecture of Long Short-Term Memory (LSTM) networks developed by Sepp Hochreiter and Jürgen Schmidhuber (1997)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The research paper proposes a novel recurrent neural network architecture to tackle the challenge of vanishing and exploding gradients. The problem occurs when error signals are carried over multiple steps, as characterized by recurrent networks. As the number of back-propagated time steps increases, the current error is mulitiplied with the previous error, influencing it exponentially. Repeated multiplication can either lead to diverging results in case of absolute errors larger than $1$, or results converging to $0$ in case of errors in the interval $(-1, 1)$. Both circumstances make learning significantly harder, since the error cannot be consistently evaluated by the network. The concrete risk for these so-called vanishing and exlpoding values depend on the chosen activation functions.\n",
    "\n",
    "In order to circumvent the problem, the central idea presented in the paper is the use of memory cells instead of directly feeding the network with a recurrent error. Each memory cell consists of a self-connected unit with the identity activation function and weight $1.0$ (constant error carousel, CEC). This prevents the exponential influence of the back-propagated time steps, as explained above. The unit is \"guarded\" by two usual multiplicatively activated units to regulate read and write access to the CEC, so the network can learn when to \"save\" the error and when to use it for calculations. Error backpropagation is truncated at memory cells, updating incoming weights as the last step. Through the self-connected loop, however, it can \"go back in time\" indefinitely. Multiple CECs can be combined to memory blocks with multiple cells to store more information at a time. Neural networks using these memory cells are called Long Short-Term Memory networks (LSTM).\n",
    "\n",
    "Limitations of LSTM primarily resemble restrictions to the type of problem it can solve. Due to the truncated error backpropagation, the memory cells cannot influence each other. Thus, delayed XOR problems cannot be solved. Also, LSTM lacks precision in the prediction of an exact time step. While it is successful in memorizing and facilitating a signal, it cannot count the steps to determine the exact age of the signal. The potential abuse of memory cells as usual nodes has been explained to be solvable by various techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innovation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time the work was published, vanishing and exploding errors lead to significant limitations of recurrent neural networks, which gave them little advatage to feed-forward networks. Comparisons of neural network architectures (including RNNs) show that the least of them can cope with very long time lags due to these effects. LSTM can not only successfully cope with them using its memory cells with constant error carousels, but also finds solutions faster with more reliable results. Truncating the backpropagation in these cells does not do harm to the result. This also leads to a backpropagation complexity of $O(1)$, which was unprecedented for RNNs. Furthermore, the use of gate units shows, that a network can learn by itself to regulate access to certain nodes.\n",
    "\n",
    "The problem was discovered in 1991 (Hochreiter 1991) and LSTM was the first solution to the problem. For more than 20 years now, it is still actively used in relevant applications, such as text generation (including image captioning), music composition, handwriting recognition and generation, and language translation (Microsoft Captionbot, Johnson 2017, Graves 2014, Nvidia 2015). Additionally, it is an ongoing topic of research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The technical quality of the work is generally high. The mathematical foundations of the developed concept are sound and the result proven to work. Edge cases have been considered and potential flaws identified, while also proposing mitigating techniques. The model has been tested and compared on both conventional and carefully designed own problems. For comparison, multiple representative network types have been implemented as described in the corresponding papers and run against LSTM in an own setup, ensuring comparability of the results. After its inital version, each experiment was modified to be harder to solve, in order to push the capability limits.\n",
    "\n",
    "Despite the great benchmark setup, there are also some critical remarks. Fistly, the problems described and tested on are merely artificial problems. They show the capabilities of each architecture in very pure way, yet testing a method on real data can be completely different. Also, the network architecture seems to have been chosen arbitrarily. This could, however, be due to lacking knowledge on my part about the problem complexities. The network topology has some level of explanation for each experiment, as e.g. to the solution of the abuse problem and comparable number of weights to the other networks. There were also different hyperparameters tested and compared. However inconclusive, this is fairly common in the machine learning domain and it is also mentioned in the conclusion, that LSTM is rather robust against hyperparameter differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application and X-factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work described in the paper was very promising at the time it was published and LSTM is still a widely used network architecture. It effectively prevents errors in recurrent models from vanishing or diverging, even for very large time lags. However, as models grow much bigger and deeper in modern times, new problems with recurrent networks emerge, also affecting LSTM. The problem that memory cells solve, re-appears on a macro-scale when the network has backpropagate thousnads of time steps. Thus even LSTMs do not have unlimited capability as to how long they can remember. Another issue with recurrent neural networks in general is the increased computational power that is needed in comparison to other network architectures. This becomes increasingly important in the era of cloud computing. Convolutional neural networks have been shown to outperform LSTM in learning speed while using fewer parameters (Elbayad et al. 2018, Bai et al. 2018). LSTM tries to mitigate the problem of vanishing gradients, whereas CNNs avoid the problem altogether by not having a sequential backprapagation path. Other solutions to the problem are attention-based, like the Transformer network (VAS et al. 2017) or hierarchical attention architectures with a maximum backpropagation path length of $log(n)$ where $n$ is the number of attention layers (Yang et al. 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reading the paper, I found it reasonably easy to follow and well structured. Subsections and paragraphs were logically organized and labelled, easing orientation while reading as well as searching for specific information. Mathematical conclusions were explained intuitively in the text and the author pointed to further readings for a deeper understanding and backgrounds. Additionally, the appendix provides prooves to complexity and correctness, as well as gives more detailed algorithm steps and example functions for an implementation. Furthermore, illustrations of the developed concept were given and appropriately designed to accompany the explanations. For a better understanding, the test setups could have been clarified better in terms of architectural design decisions as already mentioned before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[HOC97][0]: Hochreiter, S, Schmidhuber, J. 'Long Short-Term Memory'.\n",
    "\n",
    "[HOC91][1]: Hochreiter, S. 1991, 'Untersuchungen zu dynamischen neuronalen Netzen', Diplomarbeit, Technische Universität München.\n",
    "\n",
    "[ELB18][2] Elbayad, M, Besacier, L, Verbeek, J. 2018, 'Pervasive Attention: 2D Convolutional Neural Networksfor Sequence-to-Sequence Prediction'. *CoNLL 2018 - Conference on Computational Natural Language Learning*, p.97–107.\n",
    "\n",
    "[BAI18][3]: Bai, S., Kolter J. Z., Koltun, V. 2018. 'An Empirical Evaluation of Generic Convolutional and Recurrent Networksfor Sequence Modeling'.\n",
    "\n",
    "[VAS17][4]: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez A. N., Kaiser L., Polozukhin, I. 2017. 'Attention Is All You Need'. *31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA*.\n",
    "\n",
    "[YAN16][5]: Yang, Z., Yang, D., Dyer, C., He, X. Smola, A. Hovy, E. 2016. 'Hierarchical Attention Networks for Document Classification'. Carnegie Mellon University, Microsoft Research, Redmond.\n",
    "\n",
    "[KYU15][6]: Kyunghyun, C. 2015. 'Statistical Machine Translation'. NVIDIA Developer Blog, viewed 27 August 2019, https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/.\n",
    "\n",
    "[GRA14][7]: Graves A. 2014. 'Generating Sequences With Recurrent Neural Networks', University of Toronto.\n",
    "\n",
    "[MIC][8]: Microsoft Captionbot\n",
    "\n",
    "[JOH17][9]: Johnson, D. D. 2017. 'Generating Polyphonic Music Using Tied Parallel Networks', *International Conference on Evolutionary and Biologically Inspired Music and Art*, pp 128-143.\n",
    "\n",
    "[NHS19][10]: NHS England, 2019. 'The Topol Review', Health Education England.\n",
    "\n",
    "[0]:https://www.bioinf.jku.at/publications/older/2604.pdf\n",
    "[1]:http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf\n",
    "[2]:https://arxiv.org/pdf/1808.03867.pdf\n",
    "[3]:https://arxiv.org/pdf/1803.01271.pdf\n",
    "[4]:https://arxiv.org/pdf/1706.03762.pdf\n",
    "[5]:http://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
    "[6]:https://devblogs.nvidia.com/introduction-neural-machine-translation-with-gpus/\n",
    "[7]:https://arxiv.org/pdf/1308.0850.pdf\n",
    "[8]:https://www.captionbot.ai/\n",
    "[9]:https://link.springer.com/chapter/10.1007/978-3-319-55750-2_9\n",
    "[10]:https://topol.hee.nhs.uk/wp-content/uploads/HEE-Topol-Review-2019.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
